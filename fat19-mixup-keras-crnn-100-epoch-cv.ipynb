{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from psutil import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imgaug import augmenters as iaa\n",
    "#from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 80\n",
    "SIZE=128\n",
    "checkpoint_file = ['model_best1.h5', 'model_best2.h5', 'model_best3.h5','model_best4.h5','model_best5.h5']\n",
    "# See Version40 for 3 snapshots (or you can use only 1 which is normal run)\n",
    "EPOCHS = 100 #150 for inception, 100 for xception\n",
    "TTA = 19 #Number of test-time augmentation\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "LR = 3e-4\n",
    "PATIENCE = 10 #ReduceOnPlateau option\n",
    "LR_FACTOR = 0.8 #ReduceOnPlateau option\n",
    "CURATED_ONLY = False # use only curated data for training\n",
    "TRAIN_AUGMENT = True # use augmentation for training data?\n",
    "VALID_AUGMENT = False\n",
    "MODEL = 'crnn' #'cnn8th' # choose among 'xception', 'inception', 'mobile', 'crnn', 'simple'\n",
    "SEED = 520\n",
    "\n",
    "USE_MIXUP = True\n",
    "MIXUP_PROB = 0.275\n",
    "\n",
    "# No K-Fold implementation yet\n",
    "# NUM_K_FOLDS = 5 # how many folds (K) you gonna splits\n",
    "# NUM_MODEL_RUN = 5 # how many models (<= K) you gonna train [e.g. set to 1 for a simple train/test split]\n",
    "\n",
    "# if use BCEwithLogits loss, use Activation = 'linear' only\n",
    "ACTIVATION = 'linear' \n",
    "# ACTIVATION = 'softmax'\n",
    "# ACTIVATION = 'sigmoid'\n",
    "\n",
    "# LOSS = 'categorical_crossentropy'\n",
    "# LOSS = 'binary_crossentropy' \n",
    "LOSS = 'BCEwithLogits' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from official code https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8#scrollTo=cRCaCIb9oguU\n",
    "def _one_sample_positive_class_precisions(scores, truth):\n",
    "    \"\"\"Calculate precisions for each true class for a single sample.\n",
    "\n",
    "    Args:\n",
    "      scores: np.array of (num_classes,) giving the individual classifier scores.\n",
    "      truth: np.array of (num_classes,) bools indicating which classes are true.\n",
    "\n",
    "    Returns:\n",
    "      pos_class_indices: np.array of indices of the true classes for this sample.\n",
    "      pos_class_precisions: np.array of precisions corresponding to each of those\n",
    "        classes.\n",
    "    \"\"\"\n",
    "    num_classes = scores.shape[0]\n",
    "    pos_class_indices = np.flatnonzero(truth > 0)\n",
    "    # Only calculate precisions if there are some true classes.\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "    # Retrieval list of classes for this sample.\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "    # class_rankings[top_scoring_class_index] == 0 etc.\n",
    "    class_rankings = np.zeros(num_classes, dtype=np.int)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "    # Which of these is a true label?\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "    # Num hits for every truncated retrieval list.\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "\n",
    "def calculate_per_class_lwlrap(truth, scores):\n",
    "    \"\"\"Calculate label-weighted label-ranking average precision.\n",
    "\n",
    "    Arguments:\n",
    "      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n",
    "        of presence of that class in that sample.\n",
    "      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n",
    "        test's real-valued score for each class for each sample.\n",
    "\n",
    "    Returns:\n",
    "      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n",
    "        class.\n",
    "      weight_per_class: np.array of (num_classes,) giving the prior of each\n",
    "        class within the truth labels.  Then the overall unbalanced lwlrap is\n",
    "        simply np.sum(per_class_lwlrap * weight_per_class)\n",
    "    \"\"\"\n",
    "    assert truth.shape == scores.shape\n",
    "    num_samples, num_classes = scores.shape\n",
    "    # Space to store a distinct precision value for each class on each sample.\n",
    "    # Only the classes that are true for each sample will be filled in.\n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = (\n",
    "            _one_sample_positive_class_precisions(scores[sample_num, :],\n",
    "                                                  truth[sample_num, :]))\n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n",
    "            precision_at_hits)\n",
    "    labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "    # Form average of each column, i.e. all the precisions assigned to labels in\n",
    "    # a particular class.\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                        np.maximum(1, labels_per_class))\n",
    "    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n",
    "    #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n",
    "    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n",
    "    #                = np.sum(per_class_lwlrap * weight_per_class)\n",
    "    return per_class_lwlrap, weight_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "# from https://www.kaggle.com/rio114/keras-cnn-with-lwlrap-evaluation/\n",
    "def tf_one_sample_positive_class_precisions(y_true, y_pred) :\n",
    "    num_samples, num_classes = y_pred.shape\n",
    "    \n",
    "    # find true labels\n",
    "    pos_class_indices = tf.where(y_true > 0) \n",
    "    \n",
    "    # put rank on each element\n",
    "    retrieved_classes = tf.nn.top_k(y_pred, k=num_classes).indices\n",
    "    sample_range = tf.zeros(shape=tf.shape(tf.transpose(y_pred)), dtype=tf.int32)\n",
    "    sample_range = tf.add(sample_range, tf.range(tf.shape(y_pred)[0], delta=1))\n",
    "    sample_range = tf.transpose(sample_range)\n",
    "    sample_range = tf.reshape(sample_range, (-1,num_classes*tf.shape(y_pred)[0]))\n",
    "    retrieved_classes = tf.reshape(retrieved_classes, (-1,num_classes*tf.shape(y_pred)[0]))\n",
    "    retrieved_class_map = tf.concat((sample_range, retrieved_classes), axis=0)\n",
    "    retrieved_class_map = tf.transpose(retrieved_class_map)\n",
    "    retrieved_class_map = tf.reshape(retrieved_class_map, (tf.shape(y_pred)[0], num_classes, 2))\n",
    "    \n",
    "    class_range = tf.zeros(shape=tf.shape(y_pred), dtype=tf.int32)\n",
    "    class_range = tf.add(class_range, tf.range(num_classes, delta=1))\n",
    "    \n",
    "    class_rankings = tf.scatter_nd(retrieved_class_map,\n",
    "                                          class_range,\n",
    "                                          tf.shape(y_pred))\n",
    "    \n",
    "    #pick_up ranks\n",
    "    num_correct_until_correct = tf.gather_nd(class_rankings, pos_class_indices)\n",
    "\n",
    "    # add one for division for \"presicion_at_hits\"\n",
    "    num_correct_until_correct_one = tf.add(num_correct_until_correct, 1) \n",
    "    num_correct_until_correct_one = tf.cast(num_correct_until_correct_one, tf.float32)\n",
    "    \n",
    "    # generate tensor [num_sample, predict_rank], \n",
    "    # top-N predicted elements have flag, N is the number of positive for each sample.\n",
    "    sample_label = pos_class_indices[:, 0]   \n",
    "    sample_label = tf.reshape(sample_label, (-1, 1))\n",
    "    sample_label = tf.cast(sample_label, tf.int32)\n",
    "    \n",
    "    num_correct_until_correct = tf.reshape(num_correct_until_correct, (-1, 1))\n",
    "    retrieved_class_true_position = tf.concat((sample_label, \n",
    "                                               num_correct_until_correct), axis=1)\n",
    "    retrieved_pos = tf.ones(shape=tf.shape(retrieved_class_true_position)[0], dtype=tf.int32)\n",
    "    retrieved_class_true = tf.scatter_nd(retrieved_class_true_position, \n",
    "                                         retrieved_pos, \n",
    "                                         tf.shape(y_pred))\n",
    "    # cumulate predict_rank\n",
    "    retrieved_cumulative_hits = tf.cumsum(retrieved_class_true, axis=1)\n",
    "\n",
    "    # find positive position\n",
    "    pos_ret_indices = tf.where(retrieved_class_true > 0)\n",
    "\n",
    "    # find cumulative hits\n",
    "    correct_rank = tf.gather_nd(retrieved_cumulative_hits, pos_ret_indices)  \n",
    "    correct_rank = tf.cast(correct_rank, tf.float32)\n",
    "\n",
    "    # compute presicion\n",
    "    precision_at_hits = tf.truediv(correct_rank, num_correct_until_correct_one)\n",
    "\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "def tf_lwlrap(y_true, y_pred):\n",
    "    num_samples, num_classes = y_pred.shape\n",
    "    pos_class_indices, precision_at_hits = (tf_one_sample_positive_class_precisions(y_true, y_pred))\n",
    "    pos_flgs = tf.cast(y_true > 0, tf.int32)\n",
    "    labels_per_class = tf.reduce_sum(pos_flgs, axis=0)\n",
    "    weight_per_class = tf.truediv(tf.cast(labels_per_class, tf.float32),\n",
    "                                  tf.cast(tf.reduce_sum(labels_per_class), tf.float32))\n",
    "    sum_precisions_by_classes = tf.zeros(shape=(num_classes), dtype=tf.float32)  \n",
    "    class_label = pos_class_indices[:,1]\n",
    "    sum_precisions_by_classes = tf.unsorted_segment_sum(precision_at_hits,\n",
    "                                                        class_label,\n",
    "                                                       num_classes)\n",
    "    labels_per_class = tf.cast(labels_per_class, tf.float32)\n",
    "    labels_per_class = tf.add(labels_per_class, 1e-7)\n",
    "    per_class_lwlrap = tf.truediv(sum_precisions_by_classes,\n",
    "                                  tf.cast(labels_per_class, tf.float32))\n",
    "    out = tf.cast(tf.tensordot(per_class_lwlrap, weight_per_class, axes=1), dtype=tf.float32)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as k\n",
    "def BCEwithLogits(y_true, y_pred):\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred, from_logits=True), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = Path('../input/freesound-audio-tagging-2019')\n",
    "preprocessed_dir = Path('../input/fat2019_prep_mels1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvs = {\n",
    "    'train_curated': dataset_dir / 'train_curated.csv',\n",
    "    #'train_noisy': dataset_dir / 'train_noisy.csv',\n",
    "    'train_noisy': preprocessed_dir / 'trn_noisy_best50s.csv',\n",
    "    'sample_submission': dataset_dir / 'sample_submission.csv',\n",
    "}\n",
    "\n",
    "dataset = {\n",
    "    'train_curated': dataset_dir / 'train_curated',\n",
    "    'train_noisy': dataset_dir / 'train_noisy',\n",
    "    'test': dataset_dir / 'test',\n",
    "}\n",
    "\n",
    "mels = {\n",
    "    'train_curated': preprocessed_dir / 'mels_train_curated.pkl',\n",
    "    'train_noisy': preprocessed_dir / 'mels_trn_noisy_best50s.pkl',\n",
    "    'test': preprocessed_dir / 'mels_test.pkl',  # NOTE: this data doesn't work at 2nd stage\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>labels</th>\n",
       "      <th>singled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0006ae4e.wav</td>\n",
       "      <td>Bark</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0019ef41.wav</td>\n",
       "      <td>Raindrop</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ec0ad.wav</td>\n",
       "      <td>Finger_snapping</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0026c7cb.wav</td>\n",
       "      <td>Run</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0026f116.wav</td>\n",
       "      <td>Finger_snapping</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fname           labels singled\n",
       "0  0006ae4e.wav             Bark     NaN\n",
       "1  0019ef41.wav         Raindrop     NaN\n",
       "2  001ec0ad.wav  Finger_snapping     NaN\n",
       "3  0026c7cb.wav              Run     NaN\n",
       "4  0026f116.wav  Finger_snapping     NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_curated = pd.read_csv(csvs['train_curated'])\n",
    "train_noisy = pd.read_csv(csvs['train_noisy'])\n",
    "if CURATED_ONLY:\n",
    "    train_df = train_curated\n",
    "else:\n",
    "    train_df = pd.concat([train_curated, train_noisy], sort=True, ignore_index=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>Accelerating_and_revving_and_vroom</th>\n",
       "      <th>Accordion</th>\n",
       "      <th>Acoustic_guitar</th>\n",
       "      <th>Applause</th>\n",
       "      <th>Bark</th>\n",
       "      <th>Bass_drum</th>\n",
       "      <th>Bass_guitar</th>\n",
       "      <th>Bathtub_(filling_or_washing)</th>\n",
       "      <th>Bicycle_bell</th>\n",
       "      <th>Burping_and_eructation</th>\n",
       "      <th>Bus</th>\n",
       "      <th>Buzz</th>\n",
       "      <th>Car_passing_by</th>\n",
       "      <th>Cheering</th>\n",
       "      <th>Chewing_and_mastication</th>\n",
       "      <th>Child_speech_and_kid_speaking</th>\n",
       "      <th>Chink_and_clink</th>\n",
       "      <th>Chirp_and_tweet</th>\n",
       "      <th>Church_bell</th>\n",
       "      <th>Clapping</th>\n",
       "      <th>Computer_keyboard</th>\n",
       "      <th>Crackle</th>\n",
       "      <th>Cricket</th>\n",
       "      <th>Crowd</th>\n",
       "      <th>Cupboard_open_or_close</th>\n",
       "      <th>Cutlery_and_silverware</th>\n",
       "      <th>Dishes_and_pots_and_pans</th>\n",
       "      <th>Drawer_open_or_close</th>\n",
       "      <th>Drip</th>\n",
       "      <th>Electric_guitar</th>\n",
       "      <th>Fart</th>\n",
       "      <th>Female_singing</th>\n",
       "      <th>Female_speech_and_woman_speaking</th>\n",
       "      <th>Fill_(with_liquid)</th>\n",
       "      <th>Finger_snapping</th>\n",
       "      <th>Frying_(food)</th>\n",
       "      <th>Gasp</th>\n",
       "      <th>Glockenspiel</th>\n",
       "      <th>Gong</th>\n",
       "      <th>...</th>\n",
       "      <th>Harmonica</th>\n",
       "      <th>Hi-hat</th>\n",
       "      <th>Hiss</th>\n",
       "      <th>Keys_jangling</th>\n",
       "      <th>Knock</th>\n",
       "      <th>Male_singing</th>\n",
       "      <th>Male_speech_and_man_speaking</th>\n",
       "      <th>Marimba_and_xylophone</th>\n",
       "      <th>Mechanical_fan</th>\n",
       "      <th>Meow</th>\n",
       "      <th>Microwave_oven</th>\n",
       "      <th>Motorcycle</th>\n",
       "      <th>Printer</th>\n",
       "      <th>Purr</th>\n",
       "      <th>Race_car_and_auto_racing</th>\n",
       "      <th>Raindrop</th>\n",
       "      <th>Run</th>\n",
       "      <th>Scissors</th>\n",
       "      <th>Screaming</th>\n",
       "      <th>Shatter</th>\n",
       "      <th>Sigh</th>\n",
       "      <th>Sink_(filling_or_washing)</th>\n",
       "      <th>Skateboard</th>\n",
       "      <th>Slam</th>\n",
       "      <th>Sneeze</th>\n",
       "      <th>Squeak</th>\n",
       "      <th>Stream</th>\n",
       "      <th>Strum</th>\n",
       "      <th>Tap</th>\n",
       "      <th>Tick-tock</th>\n",
       "      <th>Toilet_flush</th>\n",
       "      <th>Traffic_noise_and_roadway_noise</th>\n",
       "      <th>Trickle_and_dribble</th>\n",
       "      <th>Walk_and_footsteps</th>\n",
       "      <th>Water_tap_and_faucet</th>\n",
       "      <th>Waves_and_surf</th>\n",
       "      <th>Whispering</th>\n",
       "      <th>Writing</th>\n",
       "      <th>Yell</th>\n",
       "      <th>Zipper_(clothing)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000ccb97.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0012633b.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ed5f1.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00294be0.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003fde7a.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fname        ...          Zipper_(clothing)\n",
       "0  000ccb97.wav        ...                          0\n",
       "1  0012633b.wav        ...                          0\n",
       "2  001ed5f1.wav        ...                          0\n",
       "3  00294be0.wav        ...                          0\n",
       "4  003fde7a.wav        ...                          0\n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(csvs['sample_submission'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accelerating_and_revving_and_vroom',\n",
       " 'Accordion',\n",
       " 'Acoustic_guitar',\n",
       " 'Applause',\n",
       " 'Bark',\n",
       " 'Bass_drum',\n",
       " 'Bass_guitar',\n",
       " 'Bathtub_(filling_or_washing)',\n",
       " 'Bicycle_bell',\n",
       " 'Burping_and_eructation']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = test_df.columns[1:].tolist()\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part is from the Kernel:Keras 2D model, 5-fold, log_specgram.\n",
    "### Use it to solve multiclass problem of cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_unique_labels(all_labels):\n",
    "    label_dict = {}\n",
    "    all_labels_set = []\n",
    "    first_labels_set = []\n",
    "    for labs in all_labels:\n",
    "        lab = labs.split(',')\n",
    "        for l in lab:\n",
    "            if l in label_dict:\n",
    "                label_dict[l] = label_dict[l]  + 1\n",
    "            else:\n",
    "                label_dict[l]= 0\n",
    "\n",
    "        all_labels_set.append(set(lab))\n",
    "        first_labels_set.append(lab[0])\n",
    "    classes = list(label_dict.keys())\n",
    "    \n",
    "    return label_dict, classes, all_labels_set, first_labels_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8970\n"
     ]
    }
   ],
   "source": [
    "label_dict, classes, all_labels_set, first_labels_set = create_unique_labels(train_df.labels)\n",
    "files = train_df.fname\n",
    "print (len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "binarize = MultiLabelBinarizer(classes=classes)\n",
    "encode = LabelEncoder()\n",
    "Y_split = encode.fit_transform(first_labels_set)\n",
    "Y = binarize.fit_transform(all_labels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_train = np.zeros((len(train_df), num_classes)).astype(int)\n",
    "#for i, row in enumerate(train_df['labels'].str.split(',')):\n",
    "#    for label in row:\n",
    "#        idx = labels.index(label)\n",
    "#        y_train[i, idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8970, 1120)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(mels['train_curated'], 'rb') as curated, open(mels['train_noisy'], 'rb') as noisy:\n",
    "    x_train = pickle.load(curated)\n",
    "    if CURATED_ONLY == False:\n",
    "        x_train.extend(pickle.load(noisy))\n",
    "\n",
    "with open(mels['test'], 'rb') as test:\n",
    "    x_test = pickle.load(test)\n",
    "    \n",
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 448, 3)\n",
      "(128, 128, 3) \n",
      "\n",
      "(128, 131, 3)\n",
      "(128, 1021, 3) \n",
      "\n",
      "(128, 128, 3)\n",
      "(128, 300, 3) \n",
      "\n",
      "(128, 1623, 3)\n",
      "(128, 1146, 3) \n",
      "\n",
      "(128, 128, 3)\n",
      "(128, 1442, 3) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ii in range(5):\n",
    "    print(x_train[ii].shape) #x_train is of shape (TRAIN_NUM,128,LEN,3) [4D Tensor]\n",
    "    print(x_test[ii].shape,'\\n')  #x_test of shape (TEST_NUM,128,LEN,3) [4D Tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_to_trn(x_train,Y,IDarray):\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i, ID in enumerate(IDarray):\n",
    "              \n",
    "        xx = x_train[ID].copy()\n",
    "        X.append(xx)    \n",
    "        y.append(Y[ID, :])\n",
    "            \n",
    "    y = np.array(y, dtype='float32')\n",
    "#        X = np.expand_dims(np.array(X), -1)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.utils import shuffle\n",
    "#train_X11,train_y11 = shuffle(xtrn,ytrn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input as preprocess_inception\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.applications.mobilenet_v2 import preprocess_input as preprocess_mobile\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input as preprocess_xception\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from sklearn.utils import shuffle\n",
    "def create_model_inception(n_out=NUM_CLASSES):\n",
    "\n",
    "    base_model =InceptionV3(weights=None, include_top=False)\n",
    "    \n",
    "    x0 = base_model.output\n",
    "    x1 = GlobalAveragePooling2D()(x0)\n",
    "    x2 = GlobalMaxPooling2D()(x0)\n",
    "    x = Concatenate()([x1,x2])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    \n",
    "    predictions = Dense(n_out, activation=ACTIVATION)(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_xception(n_out=NUM_CLASSES):\n",
    "\n",
    "    base_model = Xception(weights=None, include_top=False)\n",
    "    \n",
    "    x0 = base_model.output\n",
    "    x1 = GlobalAveragePooling2D()(x0)\n",
    "    x2 = GlobalMaxPooling2D()(x0)\n",
    "    x = Concatenate()([x1,x2])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(0.3)(x)\n",
    "    \n",
    "    predictions = Dense(n_out, activation=ACTIVATION)(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_mobile(n_out=NUM_CLASSES):\n",
    "\n",
    "    base_model =MobileNetV2(weights=None, include_top=False)\n",
    "    \n",
    "    x0 = base_model.output\n",
    "    x1 = GlobalAveragePooling2D()(x0)\n",
    "    x2 = GlobalMaxPooling2D()(x0)\n",
    "    x = Concatenate()([x1,x2])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(0.25)(x)\n",
    "\n",
    "    \n",
    "    predictions = Dense(n_out, activation=ACTIVATION)(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_simple_block(x, n_filters):\n",
    "    \n",
    "    x = Convolution2D(n_filters, (3,1), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Convolution2D(n_filters, (3,1), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = AveragePooling2D()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def create_model_simplecnn(n_out=NUM_CLASSES):\n",
    "    \n",
    "    inp = Input(shape=(128,128,3))\n",
    "#     inp = Input(shape=(None,None,3))\n",
    "    x = conv_simple_block(inp,64)\n",
    "    x = conv_simple_block(x,128)\n",
    "    x = conv_simple_block(x,256)\n",
    "    x = conv_simple_block(x,128)\n",
    "    \n",
    "#     x1 = GlobalAveragePooling2D()(x)\n",
    "#     x2 = GlobalMaxPooling2D()(x)\n",
    "#     x = Add()([x1,x2])\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(128, activation='linear')(x)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(n_out, activation=ACTIVATION)(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_of_lambda(input_shape):\n",
    "    return (input_shape[0], input_shape[2], input_shape[3])\n",
    "\n",
    "def my_max(x):\n",
    "    return K.max(x, axis=1, keepdims=False)\n",
    "\n",
    "def crnn_simple_block(x, n_filters):\n",
    "    \n",
    "    x = Convolution2D(n_filters, (3,1), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Convolution2D(n_filters, (3,1), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def create_model_crnn(n_out=NUM_CLASSES):\n",
    "    \n",
    "#     inp = Input(shape=(128,128,3))\n",
    "    inp = Input(shape=(128,None,3))\n",
    "    x = crnn_simple_block(inp,64)\n",
    "    x = crnn_simple_block(x,128)\n",
    "    x = crnn_simple_block(x,256)\n",
    "    \n",
    "    # eliminate the frequency dimension, x = (batch, time, channels)\n",
    "    x = Lambda(my_max, output_shape=output_of_lambda)(x)\n",
    "    \n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "#     x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation='linear')(x)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(n_out, activation=ACTIVATION)(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from the 8th solution in 2018 competition\n",
    "# https://github.com/sainathadapa/kaggle-freesound-audio-tagging\n",
    "def create_model_cnn8th(n_out=NUM_CLASSES):\n",
    "    regu=0\n",
    "    inp = Input(shape=(128,128,3))\n",
    "\n",
    "    x = Conv2D(48, 11,  strides=(1,1),kernel_initializer='he_uniform', activation='relu', padding='same',kernel_regularizer=regularizers.l2(regu))(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(48, 11,  strides=(2,3),kernel_initializer='he_uniform', activation='relu', padding='same',kernel_regularizer=regularizers.l2(regu))(x)\n",
    "    x = MaxPooling2D(3, strides=(1,2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(128, 5, strides=(1,1),kernel_initializer='he_uniform', activation='relu', padding='same',kernel_regularizer=regularizers.l2(regu))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, 5, strides=(2,3),kernel_initializer='he_uniform', activation='relu', padding='same',kernel_regularizer=regularizers.l2(regu))(x)\n",
    "    x = MaxPooling2D(3, strides=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(192, 3, strides=1,kernel_initializer='he_uniform', activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(192, 3, strides=1,kernel_initializer='he_uniform', activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, 3, strides=1,kernel_initializer='he_uniform', activation='relu', padding='same',kernel_regularizer=regularizers.l2(regu))(x)\n",
    "    x = MaxPooling2D(3, strides=(1,2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    predictions = Dense(n_out, activation=ACTIVATION)(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "crnn\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, None, 3)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, None, 64)     640       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, None, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, None, 64)     12352     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, None, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, None, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, None, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, None, 128)     24704     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, None, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, None, 128)     49280     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64, None, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, None, 128)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, None, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, None, 256)     98560     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, None, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, None, 256)     196864    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, None, 256)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, None, 256)     0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, None, 256)     0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 256)         296448    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (None, 128)               128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 80)                10320     \n",
      "=================================================================\n",
      "Total params: 722,704\n",
      "Trainable params: 722,448\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "'''Choose your model here'''\n",
    "if MODEL == 'xception':\n",
    "    preprocess_input = preprocess_xception\n",
    "    model = create_model_xception(n_out=NUM_CLASSES)\n",
    "elif MODEL == 'inception':\n",
    "    preprocess_input = preprocess_inception\n",
    "    model = create_model_inception(n_out=NUM_CLASSES)\n",
    "elif MODEL == 'mobile':\n",
    "    preprocess_input = preprocess_mobile\n",
    "    model = create_model_mobile(n_out=NUM_CLASSES)\n",
    "elif MODEL == 'crnn':\n",
    "    preprocess_input = preprocess_mobile\n",
    "    model = create_model_crnn(n_out=NUM_CLASSES)\n",
    "elif MODEL == 'cnn8th':\n",
    "    preprocess_input = preprocess_mobile\n",
    "    model = create_model_cnn8th(n_out=NUM_CLASSES)\n",
    "else:\n",
    "    preprocess_input = preprocess_mobile\n",
    "    model = create_model_simplecnn(n_out=NUM_CLASSES)\n",
    "\n",
    "print(MODEL)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,) [0.71861741]\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "xx = np.random.rand(1)\n",
    "print(xx.shape,xx)\n",
    "\n",
    "xx = np.random.rand(1,1)\n",
    "print(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you want, you can try more advanced augmentation like this\n",
    "augment_img = iaa.Sequential([\n",
    "#         iaa.ContrastNormalization((0.9, 1.1)),\n",
    "#         iaa.Multiply((0.9, 1.1), per_channel=0.2),\n",
    "#        iaa.Fliplr(0.5),\n",
    "#         iaa.GaussianBlur(sigma=(0, 0.1)),\n",
    "#         iaa.Affine( # x-shift\n",
    "#             translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.0, 0.0)},\n",
    "#         ),\n",
    "        iaa.CoarseDropout(0.12,size_percent=0.05) # see examples : https://github.com/aleju/imgaug\n",
    "            ], random_order=True)\n",
    "\n",
    "\n",
    "\n",
    "# Or you can choose this simplest augmentation (like pytorch version)\n",
    "# augment_img = iaa.Fliplr(0.5)\n",
    "\n",
    "# This is my ugly modification; sorry about that\n",
    "class FATTrainDataset(Sequence):\n",
    "\n",
    "    def mix_up(x, y):\n",
    "        x = np.array(x, np.float32)\n",
    "        lam = np.random.beta(1.0, 1.0)\n",
    "        ori_index = np.arange(int(len(x)))\n",
    "        index_array = np.arange(int(len(x)))\n",
    "        np.random.shuffle(index_array)        \n",
    "        \n",
    "        mixed_x = lam * x[ori_index] + (1 - lam) * x[index_array]\n",
    "        mixed_y = lam * y[ori_index] + (1 - lam) * y[index_array]\n",
    "        \n",
    "        return mixed_x, mixed_y\n",
    "    \n",
    "    def getitem(image):\n",
    "        # crop 2sec\n",
    "\n",
    "        base_dim, time_dim, _ = image.shape\n",
    "        crop = random.randint(0, time_dim - base_dim)\n",
    "        image = image[:,crop:crop+base_dim,:]\n",
    "\n",
    "        image = preprocess_input(image)\n",
    "        \n",
    "#         label = self.labels[idx]\n",
    "        return image\n",
    "    def create_generator(train_X, train_y, batch_size, shape, augument=False, shuffling=False, test_data=False, mixup=False, mixup_prob=0.3):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            if shuffling:\n",
    "                train_X,train_y = shuffle(train_X,train_y)\n",
    "\n",
    "            for start in range(0, len(train_y), batch_size):\n",
    "                end = min(start + batch_size, len(train_y))\n",
    "                batch_images = []\n",
    "                X_train_batch = train_X[start:end]\n",
    "                if test_data == False:\n",
    "                    batch_labels = train_y[start:end]\n",
    "                \n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = FATTrainDataset.getitem(X_train_batch[i])   \n",
    "                    if augument:\n",
    "                        image = FATTrainDataset.augment(image)\n",
    "                    batch_images.append(image)\n",
    "                \n",
    "                if (mixup and test_data == False):\n",
    "                    dice = np.random.rand(1)\n",
    "                    if dice > mixup_prob:\n",
    "                        batch_images, batch_labels =  FATTrainDataset.mix_up(batch_images, batch_labels)    \n",
    "                    \n",
    "                if test_data == False:\n",
    "                    yield np.array(batch_images, np.float32), batch_labels\n",
    "                else:\n",
    "                    yield np.array(batch_images, np.float32)\n",
    "        return image\n",
    "    \n",
    "    def augment(image):\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 57\n",
      "7152\n",
      "0.0003 10 0.8 32 True True 0.275\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/100\n",
      "224/224 [==============================] - 18s 82ms/step - loss: 0.0484 - tf_lwlrap: 0.5353 - categorical_accuracy: 0.4216 - val_loss: 0.0405 - val_tf_lwlrap: 0.6451 - val_categorical_accuracy: 0.4978\n",
      "\n",
      "Epoch 00001: val_tf_lwlrap improved from -inf to 0.64508, saving model to model_best1.h5\n",
      "Epoch 2/100\n",
      "224/224 [==============================] - 16s 69ms/step - loss: 0.0468 - tf_lwlrap: 0.5549 - categorical_accuracy: 0.4374 - val_loss: 0.0394 - val_tf_lwlrap: 0.6593 - val_categorical_accuracy: 0.5022\n",
      "\n",
      "Epoch 00002: val_tf_lwlrap improved from 0.64508 to 0.65933, saving model to model_best1.h5\n",
      "Epoch 3/100\n",
      "224/224 [==============================] - 15s 67ms/step - loss: 0.0460 - tf_lwlrap: 0.5619 - categorical_accuracy: 0.4484 - val_loss: 0.0388 - val_tf_lwlrap: 0.6732 - val_categorical_accuracy: 0.5160\n",
      "\n",
      "Epoch 00003: val_tf_lwlrap improved from 0.65933 to 0.67316, saving model to model_best1.h5\n",
      "Epoch 4/100\n",
      "224/224 [==============================] - 15s 67ms/step - loss: 0.0470 - tf_lwlrap: 0.5589 - categorical_accuracy: 0.4517 - val_loss: 0.0395 - val_tf_lwlrap: 0.6543 - val_categorical_accuracy: 0.5094\n",
      "\n",
      "Epoch 00004: val_tf_lwlrap did not improve from 0.67316\n",
      "Epoch 5/100\n",
      "224/224 [==============================] - 15s 67ms/step - loss: 0.0466 - tf_lwlrap: 0.5622 - categorical_accuracy: 0.4570 - val_loss: 0.0403 - val_tf_lwlrap: 0.6513 - val_categorical_accuracy: 0.4978\n",
      "\n",
      "Epoch 00005: val_tf_lwlrap did not improve from 0.67316\n",
      "Epoch 6/100\n",
      "224/224 [==============================] - 15s 67ms/step - loss: 0.0467 - tf_lwlrap: 0.5585 - categorical_accuracy: 0.4503 - val_loss: 0.0397 - val_tf_lwlrap: 0.6609 - val_categorical_accuracy: 0.5132\n",
      "\n",
      "Epoch 00006: val_tf_lwlrap did not improve from 0.67316\n",
      "Epoch 7/100\n",
      "224/224 [==============================] - 15s 66ms/step - loss: 0.0467 - tf_lwlrap: 0.5623 - categorical_accuracy: 0.4618 - val_loss: 0.0391 - val_tf_lwlrap: 0.6603 - val_categorical_accuracy: 0.5105\n",
      "\n",
      "Epoch 00007: val_tf_lwlrap did not improve from 0.67316\n",
      "Epoch 8/100\n",
      "224/224 [==============================] - 15s 67ms/step - loss: 0.0466 - tf_lwlrap: 0.5703 - categorical_accuracy: 0.4618 - val_loss: 0.0388 - val_tf_lwlrap: 0.6664 - val_categorical_accuracy: 0.5143\n",
      "\n",
      "Epoch 00008: val_tf_lwlrap did not improve from 0.67316\n",
      "Epoch 9/100\n",
      "224/224 [==============================] - 15s 66ms/step - loss: 0.0463 - tf_lwlrap: 0.5639 - categorical_accuracy: 0.4660 - val_loss: 0.0384 - val_tf_lwlrap: 0.6682 - val_categorical_accuracy: 0.5237\n",
      "\n",
      "Epoch 00009: val_tf_lwlrap did not improve from 0.67316\n",
      "Epoch 10/100\n",
      "224/224 [==============================] - 15s 66ms/step - loss: 0.0451 - tf_lwlrap: 0.5769 - categorical_accuracy: 0.4708 - val_loss: 0.0383 - val_tf_lwlrap: 0.6776 - val_categorical_accuracy: 0.5264\n",
      "\n",
      "Epoch 00010: val_tf_lwlrap improved from 0.67316 to 0.67763, saving model to model_best1.h5\n",
      "Epoch 11/100\n",
      "224/224 [==============================] - 15s 66ms/step - loss: 0.0467 - tf_lwlrap: 0.5644 - categorical_accuracy: 0.4517 - val_loss: 0.0401 - val_tf_lwlrap: 0.6590 - val_categorical_accuracy: 0.5182\n",
      "\n",
      "Epoch 00011: val_tf_lwlrap did not improve from 0.67763\n",
      "Epoch 12/100\n",
      "224/224 [==============================] - 15s 66ms/step - loss: 0.0476 - tf_lwlrap: 0.5570 - categorical_accuracy: 0.4389 - val_loss: 0.0400 - val_tf_lwlrap: 0.6484 - val_categorical_accuracy: 0.4950\n",
      "\n",
      "Epoch 00012: val_tf_lwlrap did not improve from 0.67763\n",
      "Epoch 13/100\n",
      "224/224 [==============================] - 15s 67ms/step - loss: 0.0472 - tf_lwlrap: 0.5661 - categorical_accuracy: 0.4538 - val_loss: 0.0402 - val_tf_lwlrap: 0.6562 - val_categorical_accuracy: 0.5039\n",
      "\n",
      "Epoch 00013: val_tf_lwlrap did not improve from 0.67763\n",
      "Epoch 14/100\n",
      "224/224 [==============================] - 15s 67ms/step - loss: 0.0462 - tf_lwlrap: 0.5671 - categorical_accuracy: 0.4570 - val_loss: 0.0400 - val_tf_lwlrap: 0.6637 - val_categorical_accuracy: 0.5165\n",
      "\n",
      "Epoch 00014: val_tf_lwlrap did not improve from 0.67763\n",
      "Epoch 15/100\n",
      "224/224 [==============================] - 15s 67ms/step - loss: 0.0469 - tf_lwlrap: 0.5568 - categorical_accuracy: 0.4556 - val_loss: 0.0406 - val_tf_lwlrap: 0.6550 - val_categorical_accuracy: 0.4923\n",
      "\n",
      "Epoch 00015: val_tf_lwlrap did not improve from 0.67763\n",
      "Epoch 16/100\n",
      "224/224 [==============================] - 15s 67ms/step - loss: 0.0459 - tf_lwlrap: 0.5749 - categorical_accuracy: 0.4676 - val_loss: 0.0391 - val_tf_lwlrap: 0.6676 - val_categorical_accuracy: 0.5160\n",
      "\n",
      "Epoch 00016: val_tf_lwlrap did not improve from 0.67763\n",
      "Epoch 17/100\n",
      "224/224 [==============================] - 15s 66ms/step - loss: 0.0466 - tf_lwlrap: 0.5699 - categorical_accuracy: 0.4549 - val_loss: 0.0388 - val_tf_lwlrap: 0.6674 - val_categorical_accuracy: 0.5116\n",
      "\n",
      "Epoch 00017: val_tf_lwlrap did not improve from 0.67763\n",
      "Epoch 18/100\n",
      "224/224 [==============================] - 15s 66ms/step - loss: 0.0458 - tf_lwlrap: 0.5796 - categorical_accuracy: 0.4788 - val_loss: 0.0391 - val_tf_lwlrap: 0.6668 - val_categorical_accuracy: 0.5209\n",
      "\n",
      "Epoch 00018: val_tf_lwlrap did not improve from 0.67763\n",
      "Epoch 19/100\n",
      "224/224 [==============================] - 15s 67ms/step - loss: 0.0468 - tf_lwlrap: 0.5657 - categorical_accuracy: 0.4598 - val_loss: 0.0395 - val_tf_lwlrap: 0.6664 - val_categorical_accuracy: 0.5028\n",
      "\n",
      "Epoch 00019: val_tf_lwlrap did not improve from 0.67763\n",
      "Epoch 20/100\n",
      "224/224 [==============================] - 15s 66ms/step - loss: 0.0462 - tf_lwlrap: 0.5714 - categorical_accuracy: 0.4655 - val_loss: 0.0396 - val_tf_lwlrap: 0.6568 - val_categorical_accuracy: 0.5022\n",
      "\n",
      "Epoch 00020: val_tf_lwlrap did not improve from 0.67763\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00024000001139938833.\n",
      "Epoch 21/100\n",
      "224/224 [==============================] - 15s 67ms/step - loss: 0.0458 - tf_lwlrap: 0.5768 - categorical_accuracy: 0.4710 - val_loss: 0.0403 - val_tf_lwlrap: 0.6499 - val_categorical_accuracy: 0.5055\n",
      "\n",
      "Epoch 00021: val_tf_lwlrap did not improve from 0.67763\n",
      "Epoch 22/100\n",
      "224/224 [==============================] - 15s 66ms/step - loss: 0.0469 - tf_lwlrap: 0.5655 - categorical_accuracy: 0.4598 - val_loss: 0.0404 - val_tf_lwlrap: 0.6591 - val_categorical_accuracy: 0.5132\n",
      "\n",
      "Epoch 00022: val_tf_lwlrap did not improve from 0.67763\n",
      "Epoch 23/100\n",
      "224/224 [==============================] - 15s 66ms/step - loss: 0.0459 - tf_lwlrap: 0.5793 - categorical_accuracy: 0.4733 - val_loss: 0.0387 - val_tf_lwlrap: 0.6816 - val_categorical_accuracy: 0.5259\n",
      "\n",
      "Epoch 00023: val_tf_lwlrap improved from 0.67763 to 0.68160, saving model to model_best1.h5\n",
      "Epoch 24/100\n",
      "224/224 [==============================] - 15s 66ms/step - loss: 0.0464 - tf_lwlrap: 0.5779 - categorical_accuracy: 0.4611 - val_loss: 0.0390 - val_tf_lwlrap: 0.6724 - val_categorical_accuracy: 0.5198\n",
      "\n",
      "Epoch 00024: val_tf_lwlrap did not improve from 0.68160\n",
      "Epoch 25/100\n",
      " 14/224 [>.............................] - ETA: 12s - loss: 0.0427 - tf_lwlrap: 0.6066 - categorical_accuracy: 0.4978"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import (ModelCheckpoint, LearningRateScheduler,\n",
    "                             EarlyStopping, ReduceLROnPlateau,CSVLogger)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "import sklearn.metrics\n",
    "#oof_y = np.zeros_like(Y, dtype='float32')\n",
    "test_Y = np.zeros((1120, 80), dtype='float32')\n",
    "\n",
    "kfold = StratifiedKFold(5)\n",
    "ifold = 0\n",
    "for train_index, valid_index in kfold.split(x_train, Y_split):\n",
    "\n",
    "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_tf_lwlrap', factor=LR_FACTOR, patience=PATIENCE, \n",
    "                                   verbose=1, mode='max', min_delta=0.0001, cooldown=2, min_lr=1e-5 )\n",
    "\n",
    "    csv_logger = CSVLogger(filename='../working/training_log'+str(ifold)+'.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(checkpoint_file[ifold], monitor='val_tf_lwlrap', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = False)\n",
    "    callbacks_list = [checkpoint, csv_logger, reduceLROnPlat]\n",
    "    x_trn,y_trn = index_to_trn(x_train,Y,train_index)\n",
    "    x_val,y_val = index_to_trn(x_train,Y,valid_index)\n",
    "    \n",
    "    # create train and valid datagens\n",
    "    train_generator = FATTrainDataset.create_generator(\n",
    "        x_trn, y_trn, BATCH_SIZE, (SIZE,SIZE,3), augument=TRAIN_AUGMENT, shuffling=True, mixup = USE_MIXUP, mixup_prob = MIXUP_PROB)\n",
    "    validation_generator = FATTrainDataset.create_generator(\n",
    "        x_val, y_val, BATCH_SIZE, (SIZE,SIZE,3), augument=VALID_AUGMENT, shuffling=False)\n",
    "    \n",
    "    train_steps = np.ceil(float(len(x_trn)) / float(BATCH_SIZE))\n",
    "    val_steps = np.ceil(float(len(x_val)) / float(BATCH_SIZE))\n",
    "    train_steps = train_steps.astype(int)\n",
    "    val_steps = val_steps.astype(int)\n",
    "    print(train_steps, val_steps)\n",
    "    print(len(x_trn))\n",
    "    \n",
    "    if LOSS=='BCEwithLogits':\n",
    "        model.compile(loss=BCEwithLogits,\n",
    "                optimizer=Adam(lr=LR),\n",
    "                metrics=[tf_lwlrap,'categorical_accuracy'])\n",
    "    else:\n",
    "        model.compile(loss=LOSS,\n",
    "                optimizer=Adam(lr=LR),\n",
    "                metrics=[tf_lwlrap,'categorical_accuracy'])\n",
    "    print(LR, PATIENCE, LR_FACTOR,BATCH_SIZE, TRAIN_AUGMENT, USE_MIXUP, MIXUP_PROB)\n",
    "    model.load_weights('../input/modelcrnn/model_best1crnn.h5')\n",
    "    \n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=val_steps,\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list)\n",
    "            \n",
    "    #TTA\n",
    "    model.load_weights(checkpoint_file[ifold])\n",
    "    validation_generator = FATTrainDataset.create_generator(\n",
    "          x_val, y_val, BATCH_SIZE, (SIZE,SIZE,3), augument=False, shuffling=False)\n",
    "    pred_val_y = model.predict_generator(validation_generator,steps=val_steps,verbose=1)    \n",
    "    for ii in range(TTA):\n",
    "        validation_generator = FATTrainDataset.create_generator(\n",
    "            x_val, y_val, BATCH_SIZE, (SIZE,SIZE,3), augument=False, shuffling=False)   \n",
    "        pred_val_y += model.predict_generator(validation_generator,steps=val_steps,verbose=1)\n",
    "    \n",
    "    train_generator = FATTrainDataset.create_generator(\n",
    "        x_trn, y_trn, BATCH_SIZE, (SIZE,SIZE,3), augument=False, shuffling=False)\n",
    "    pred_train_y = model.predict_generator(train_generator,steps=train_steps,verbose=1)\n",
    "    \n",
    "    #Predict Test Data with TTA\n",
    "    test_steps = np.ceil(float(len(x_test)) / float(BATCH_SIZE)).astype(int)   \n",
    "    model.load_weights(checkpoint_file[ifold])\n",
    "    test_generator = FATTrainDataset.create_generator(\n",
    "        x_test, x_test, BATCH_SIZE, (SIZE,SIZE,3), augument=False, shuffling=False, test_data=True)\n",
    "    pred_test_y = model.predict_generator(test_generator,steps=test_steps,verbose=1)\n",
    "\n",
    "    for ii in range(TTA):\n",
    "        test_generator = FATTrainDataset.create_generator(\n",
    "            x_test, x_test, BATCH_SIZE, (SIZE,SIZE,3), augument=False, shuffling=False, test_data=True)\n",
    "        \n",
    "        pred_test_y += model.predict_generator(test_generator,steps=test_steps,verbose=1)\n",
    "    \n",
    "    sort_idx = np.argsort(labels).astype(int)        \n",
    "    sample_sub = pd.read_csv('../input/freesound-audio-tagging-2019/sample_submission.csv')\n",
    "    test_Y_sort = pred_test_y[:, sort_idx]\n",
    "    sample_sub.iloc[:, 1:] =  test_Y_sort\n",
    "    if ifold == 0:\n",
    "        sample_subcv = sample_sub\n",
    "    sample_subcv = sample_sub + sample_subcv\n",
    "    ifold = ifold + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>Accelerating_and_revving_and_vroom</th>\n",
       "      <th>Accordion</th>\n",
       "      <th>Acoustic_guitar</th>\n",
       "      <th>Applause</th>\n",
       "      <th>Bark</th>\n",
       "      <th>Bass_drum</th>\n",
       "      <th>Bass_guitar</th>\n",
       "      <th>Bathtub_(filling_or_washing)</th>\n",
       "      <th>Bicycle_bell</th>\n",
       "      <th>Burping_and_eructation</th>\n",
       "      <th>Bus</th>\n",
       "      <th>Buzz</th>\n",
       "      <th>Car_passing_by</th>\n",
       "      <th>Cheering</th>\n",
       "      <th>Chewing_and_mastication</th>\n",
       "      <th>Child_speech_and_kid_speaking</th>\n",
       "      <th>Chink_and_clink</th>\n",
       "      <th>Chirp_and_tweet</th>\n",
       "      <th>Church_bell</th>\n",
       "      <th>Clapping</th>\n",
       "      <th>Computer_keyboard</th>\n",
       "      <th>Crackle</th>\n",
       "      <th>Cricket</th>\n",
       "      <th>Crowd</th>\n",
       "      <th>Cupboard_open_or_close</th>\n",
       "      <th>Cutlery_and_silverware</th>\n",
       "      <th>Dishes_and_pots_and_pans</th>\n",
       "      <th>Drawer_open_or_close</th>\n",
       "      <th>Drip</th>\n",
       "      <th>Electric_guitar</th>\n",
       "      <th>Fart</th>\n",
       "      <th>Female_singing</th>\n",
       "      <th>Female_speech_and_woman_speaking</th>\n",
       "      <th>Fill_(with_liquid)</th>\n",
       "      <th>Finger_snapping</th>\n",
       "      <th>Frying_(food)</th>\n",
       "      <th>Gasp</th>\n",
       "      <th>Glockenspiel</th>\n",
       "      <th>Gong</th>\n",
       "      <th>...</th>\n",
       "      <th>Harmonica</th>\n",
       "      <th>Hi-hat</th>\n",
       "      <th>Hiss</th>\n",
       "      <th>Keys_jangling</th>\n",
       "      <th>Knock</th>\n",
       "      <th>Male_singing</th>\n",
       "      <th>Male_speech_and_man_speaking</th>\n",
       "      <th>Marimba_and_xylophone</th>\n",
       "      <th>Mechanical_fan</th>\n",
       "      <th>Meow</th>\n",
       "      <th>Microwave_oven</th>\n",
       "      <th>Motorcycle</th>\n",
       "      <th>Printer</th>\n",
       "      <th>Purr</th>\n",
       "      <th>Race_car_and_auto_racing</th>\n",
       "      <th>Raindrop</th>\n",
       "      <th>Run</th>\n",
       "      <th>Scissors</th>\n",
       "      <th>Screaming</th>\n",
       "      <th>Shatter</th>\n",
       "      <th>Sigh</th>\n",
       "      <th>Sink_(filling_or_washing)</th>\n",
       "      <th>Skateboard</th>\n",
       "      <th>Slam</th>\n",
       "      <th>Sneeze</th>\n",
       "      <th>Squeak</th>\n",
       "      <th>Stream</th>\n",
       "      <th>Strum</th>\n",
       "      <th>Tap</th>\n",
       "      <th>Tick-tock</th>\n",
       "      <th>Toilet_flush</th>\n",
       "      <th>Traffic_noise_and_roadway_noise</th>\n",
       "      <th>Trickle_and_dribble</th>\n",
       "      <th>Walk_and_footsteps</th>\n",
       "      <th>Water_tap_and_faucet</th>\n",
       "      <th>Waves_and_surf</th>\n",
       "      <th>Whispering</th>\n",
       "      <th>Writing</th>\n",
       "      <th>Yell</th>\n",
       "      <th>Zipper_(clothing)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000ccb97.wav000ccb97.wav000ccb97.wav000ccb97.w...</td>\n",
       "      <td>-1208.535751</td>\n",
       "      <td>-489.764389</td>\n",
       "      <td>-456.518566</td>\n",
       "      <td>-701.913475</td>\n",
       "      <td>-586.013176</td>\n",
       "      <td>-971.171158</td>\n",
       "      <td>-1138.248474</td>\n",
       "      <td>-422.635685</td>\n",
       "      <td>-545.438194</td>\n",
       "      <td>-901.224457</td>\n",
       "      <td>-890.800995</td>\n",
       "      <td>-700.856125</td>\n",
       "      <td>-730.851051</td>\n",
       "      <td>-1107.355011</td>\n",
       "      <td>-597.197533</td>\n",
       "      <td>-859.849152</td>\n",
       "      <td>-1072.900360</td>\n",
       "      <td>-811.746292</td>\n",
       "      <td>-1119.748611</td>\n",
       "      <td>-592.484894</td>\n",
       "      <td>-1078.040924</td>\n",
       "      <td>-1046.481354</td>\n",
       "      <td>-840.570518</td>\n",
       "      <td>-585.720734</td>\n",
       "      <td>-832.899750</td>\n",
       "      <td>-1019.642700</td>\n",
       "      <td>-799.770088</td>\n",
       "      <td>-1066.703888</td>\n",
       "      <td>-1009.174347</td>\n",
       "      <td>-795.425499</td>\n",
       "      <td>-1426.266235</td>\n",
       "      <td>-1009.645905</td>\n",
       "      <td>-925.335236</td>\n",
       "      <td>-899.169113</td>\n",
       "      <td>-924.198425</td>\n",
       "      <td>-1092.138382</td>\n",
       "      <td>-913.707169</td>\n",
       "      <td>-1092.550735</td>\n",
       "      <td>-866.232391</td>\n",
       "      <td>...</td>\n",
       "      <td>-862.849030</td>\n",
       "      <td>-615.301468</td>\n",
       "      <td>-1101.262085</td>\n",
       "      <td>-947.730347</td>\n",
       "      <td>-1045.664673</td>\n",
       "      <td>-973.703995</td>\n",
       "      <td>-432.219040</td>\n",
       "      <td>-875.963364</td>\n",
       "      <td>-728.145576</td>\n",
       "      <td>-880.527367</td>\n",
       "      <td>-584.555870</td>\n",
       "      <td>-585.391609</td>\n",
       "      <td>-964.067947</td>\n",
       "      <td>-484.901978</td>\n",
       "      <td>-1243.631973</td>\n",
       "      <td>-1057.684082</td>\n",
       "      <td>-735.380890</td>\n",
       "      <td>-535.099945</td>\n",
       "      <td>-535.721672</td>\n",
       "      <td>-1069.048096</td>\n",
       "      <td>-402.511532</td>\n",
       "      <td>-884.841278</td>\n",
       "      <td>-804.297195</td>\n",
       "      <td>-961.205933</td>\n",
       "      <td>-783.791298</td>\n",
       "      <td>-825.324203</td>\n",
       "      <td>-1153.533218</td>\n",
       "      <td>-814.253647</td>\n",
       "      <td>-734.651146</td>\n",
       "      <td>-735.971741</td>\n",
       "      <td>-819.304703</td>\n",
       "      <td>-1190.782913</td>\n",
       "      <td>-903.953278</td>\n",
       "      <td>-722.317787</td>\n",
       "      <td>-645.222725</td>\n",
       "      <td>-419.578979</td>\n",
       "      <td>-655.327538</td>\n",
       "      <td>-971.755600</td>\n",
       "      <td>-716.099335</td>\n",
       "      <td>-183.735697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0012633b.wav0012633b.wav0012633b.wav0012633b.w...</td>\n",
       "      <td>-751.842697</td>\n",
       "      <td>-895.962906</td>\n",
       "      <td>-867.421051</td>\n",
       "      <td>-704.306099</td>\n",
       "      <td>-739.897842</td>\n",
       "      <td>-753.153717</td>\n",
       "      <td>-782.407387</td>\n",
       "      <td>-724.422157</td>\n",
       "      <td>-715.856247</td>\n",
       "      <td>-818.453560</td>\n",
       "      <td>-911.378540</td>\n",
       "      <td>-301.936024</td>\n",
       "      <td>-680.158508</td>\n",
       "      <td>-755.630547</td>\n",
       "      <td>-608.876564</td>\n",
       "      <td>-864.031113</td>\n",
       "      <td>-290.026196</td>\n",
       "      <td>-608.737816</td>\n",
       "      <td>-769.724442</td>\n",
       "      <td>-426.906609</td>\n",
       "      <td>-714.651573</td>\n",
       "      <td>-613.210098</td>\n",
       "      <td>-744.604477</td>\n",
       "      <td>-817.683151</td>\n",
       "      <td>-795.280212</td>\n",
       "      <td>-354.207848</td>\n",
       "      <td>-823.165154</td>\n",
       "      <td>-1072.457169</td>\n",
       "      <td>-520.129265</td>\n",
       "      <td>-889.233109</td>\n",
       "      <td>-781.946716</td>\n",
       "      <td>-753.261765</td>\n",
       "      <td>-634.641968</td>\n",
       "      <td>-903.741776</td>\n",
       "      <td>-564.437202</td>\n",
       "      <td>-747.711792</td>\n",
       "      <td>-492.885750</td>\n",
       "      <td>-745.389519</td>\n",
       "      <td>-806.032211</td>\n",
       "      <td>...</td>\n",
       "      <td>-855.618774</td>\n",
       "      <td>-610.979935</td>\n",
       "      <td>-819.299011</td>\n",
       "      <td>-661.862091</td>\n",
       "      <td>-613.951889</td>\n",
       "      <td>-812.197968</td>\n",
       "      <td>-562.205231</td>\n",
       "      <td>-426.833054</td>\n",
       "      <td>-338.949615</td>\n",
       "      <td>-698.388779</td>\n",
       "      <td>-877.470184</td>\n",
       "      <td>-531.286743</td>\n",
       "      <td>-799.881157</td>\n",
       "      <td>-697.452606</td>\n",
       "      <td>-546.853401</td>\n",
       "      <td>-672.258072</td>\n",
       "      <td>-678.940926</td>\n",
       "      <td>-513.176765</td>\n",
       "      <td>-563.296143</td>\n",
       "      <td>-691.737457</td>\n",
       "      <td>-832.854477</td>\n",
       "      <td>-789.295868</td>\n",
       "      <td>-674.096367</td>\n",
       "      <td>-694.976707</td>\n",
       "      <td>-731.491432</td>\n",
       "      <td>-836.772003</td>\n",
       "      <td>-919.809036</td>\n",
       "      <td>-833.626785</td>\n",
       "      <td>-758.823547</td>\n",
       "      <td>-662.245491</td>\n",
       "      <td>-696.038071</td>\n",
       "      <td>-697.953529</td>\n",
       "      <td>-734.918724</td>\n",
       "      <td>-835.671272</td>\n",
       "      <td>-604.741417</td>\n",
       "      <td>-926.410126</td>\n",
       "      <td>-522.782707</td>\n",
       "      <td>-840.625450</td>\n",
       "      <td>-413.779041</td>\n",
       "      <td>-662.645607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ed5f1.wav001ed5f1.wav001ed5f1.wav001ed5f1.w...</td>\n",
       "      <td>-589.730644</td>\n",
       "      <td>-983.070328</td>\n",
       "      <td>-790.755753</td>\n",
       "      <td>-290.089909</td>\n",
       "      <td>-702.252792</td>\n",
       "      <td>-1062.928558</td>\n",
       "      <td>-1084.755219</td>\n",
       "      <td>-1011.500977</td>\n",
       "      <td>-819.109970</td>\n",
       "      <td>-532.792389</td>\n",
       "      <td>-721.514160</td>\n",
       "      <td>-660.673538</td>\n",
       "      <td>-565.871483</td>\n",
       "      <td>-750.665466</td>\n",
       "      <td>-1075.867889</td>\n",
       "      <td>-520.082359</td>\n",
       "      <td>-911.394165</td>\n",
       "      <td>-363.586792</td>\n",
       "      <td>-845.800873</td>\n",
       "      <td>-509.909264</td>\n",
       "      <td>-645.876122</td>\n",
       "      <td>-968.185928</td>\n",
       "      <td>-1008.454636</td>\n",
       "      <td>-660.982269</td>\n",
       "      <td>-981.819427</td>\n",
       "      <td>-872.981171</td>\n",
       "      <td>-556.476868</td>\n",
       "      <td>-886.721634</td>\n",
       "      <td>-432.735672</td>\n",
       "      <td>-742.191345</td>\n",
       "      <td>-900.609100</td>\n",
       "      <td>-452.691719</td>\n",
       "      <td>-672.259644</td>\n",
       "      <td>-861.536667</td>\n",
       "      <td>-982.695450</td>\n",
       "      <td>-706.633469</td>\n",
       "      <td>-818.270844</td>\n",
       "      <td>-987.226364</td>\n",
       "      <td>-867.206177</td>\n",
       "      <td>...</td>\n",
       "      <td>-1168.168518</td>\n",
       "      <td>-854.510986</td>\n",
       "      <td>-665.394897</td>\n",
       "      <td>-880.903809</td>\n",
       "      <td>-729.544777</td>\n",
       "      <td>-343.049580</td>\n",
       "      <td>-863.803040</td>\n",
       "      <td>-987.546387</td>\n",
       "      <td>-859.854401</td>\n",
       "      <td>-444.200752</td>\n",
       "      <td>-993.930252</td>\n",
       "      <td>-824.201370</td>\n",
       "      <td>-336.552513</td>\n",
       "      <td>-871.256821</td>\n",
       "      <td>-426.594677</td>\n",
       "      <td>-991.930176</td>\n",
       "      <td>-549.716553</td>\n",
       "      <td>-578.379456</td>\n",
       "      <td>-557.333000</td>\n",
       "      <td>-587.638191</td>\n",
       "      <td>-1039.489441</td>\n",
       "      <td>-916.312637</td>\n",
       "      <td>-589.079903</td>\n",
       "      <td>-791.278084</td>\n",
       "      <td>-995.768951</td>\n",
       "      <td>-930.302780</td>\n",
       "      <td>-893.711655</td>\n",
       "      <td>-1043.001511</td>\n",
       "      <td>-711.920212</td>\n",
       "      <td>-283.461185</td>\n",
       "      <td>-844.166855</td>\n",
       "      <td>-772.198753</td>\n",
       "      <td>-894.641266</td>\n",
       "      <td>-871.071213</td>\n",
       "      <td>-552.119644</td>\n",
       "      <td>-928.599472</td>\n",
       "      <td>-821.762573</td>\n",
       "      <td>-799.484474</td>\n",
       "      <td>-623.461128</td>\n",
       "      <td>-570.449921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00294be0.wav00294be0.wav00294be0.wav00294be0.w...</td>\n",
       "      <td>-763.925629</td>\n",
       "      <td>-763.821388</td>\n",
       "      <td>-898.286194</td>\n",
       "      <td>-679.343735</td>\n",
       "      <td>-624.721756</td>\n",
       "      <td>-1240.844330</td>\n",
       "      <td>-1131.459015</td>\n",
       "      <td>-1384.019699</td>\n",
       "      <td>-1100.684662</td>\n",
       "      <td>-1275.179901</td>\n",
       "      <td>-1140.117432</td>\n",
       "      <td>-630.900642</td>\n",
       "      <td>-182.486727</td>\n",
       "      <td>-816.864670</td>\n",
       "      <td>-980.507538</td>\n",
       "      <td>-953.136841</td>\n",
       "      <td>-707.294075</td>\n",
       "      <td>-670.489525</td>\n",
       "      <td>-1242.622269</td>\n",
       "      <td>-780.939201</td>\n",
       "      <td>-757.040611</td>\n",
       "      <td>-597.686531</td>\n",
       "      <td>-1018.957718</td>\n",
       "      <td>-819.616730</td>\n",
       "      <td>-1080.175003</td>\n",
       "      <td>-933.453445</td>\n",
       "      <td>-1161.010773</td>\n",
       "      <td>-1261.964264</td>\n",
       "      <td>-395.375893</td>\n",
       "      <td>-938.257050</td>\n",
       "      <td>-510.607483</td>\n",
       "      <td>-747.199913</td>\n",
       "      <td>-1101.300064</td>\n",
       "      <td>-1050.622940</td>\n",
       "      <td>-1229.796143</td>\n",
       "      <td>-1018.237686</td>\n",
       "      <td>-622.615456</td>\n",
       "      <td>-58.751752</td>\n",
       "      <td>-992.587997</td>\n",
       "      <td>...</td>\n",
       "      <td>-1187.957581</td>\n",
       "      <td>-719.626381</td>\n",
       "      <td>-1070.140472</td>\n",
       "      <td>-1002.090363</td>\n",
       "      <td>-1120.891495</td>\n",
       "      <td>-1109.494049</td>\n",
       "      <td>-969.202667</td>\n",
       "      <td>-455.488232</td>\n",
       "      <td>-637.224388</td>\n",
       "      <td>-532.744484</td>\n",
       "      <td>-819.847527</td>\n",
       "      <td>-739.702866</td>\n",
       "      <td>-950.974945</td>\n",
       "      <td>-1188.047043</td>\n",
       "      <td>-739.461761</td>\n",
       "      <td>-1150.205566</td>\n",
       "      <td>-741.392097</td>\n",
       "      <td>-370.585747</td>\n",
       "      <td>-226.678919</td>\n",
       "      <td>-746.248344</td>\n",
       "      <td>-575.492683</td>\n",
       "      <td>-1082.571426</td>\n",
       "      <td>-1250.356537</td>\n",
       "      <td>-793.775551</td>\n",
       "      <td>-974.320496</td>\n",
       "      <td>-840.813553</td>\n",
       "      <td>-1046.463730</td>\n",
       "      <td>-1128.539795</td>\n",
       "      <td>-875.831146</td>\n",
       "      <td>-514.073647</td>\n",
       "      <td>-631.777603</td>\n",
       "      <td>-556.970100</td>\n",
       "      <td>-418.554161</td>\n",
       "      <td>-1158.804443</td>\n",
       "      <td>-635.476120</td>\n",
       "      <td>-971.932877</td>\n",
       "      <td>-741.481758</td>\n",
       "      <td>-1114.002258</td>\n",
       "      <td>-680.946617</td>\n",
       "      <td>-591.613785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003fde7a.wav003fde7a.wav003fde7a.wav003fde7a.w...</td>\n",
       "      <td>-948.256516</td>\n",
       "      <td>-971.734283</td>\n",
       "      <td>-940.495422</td>\n",
       "      <td>-574.359421</td>\n",
       "      <td>-1179.158890</td>\n",
       "      <td>-983.465103</td>\n",
       "      <td>-952.470535</td>\n",
       "      <td>-932.928207</td>\n",
       "      <td>-1054.553925</td>\n",
       "      <td>-1050.902527</td>\n",
       "      <td>-914.170074</td>\n",
       "      <td>-724.757202</td>\n",
       "      <td>-875.570923</td>\n",
       "      <td>-981.261368</td>\n",
       "      <td>-1078.886963</td>\n",
       "      <td>-926.196121</td>\n",
       "      <td>-733.838547</td>\n",
       "      <td>-1083.692184</td>\n",
       "      <td>-1184.151749</td>\n",
       "      <td>-843.302185</td>\n",
       "      <td>-729.529495</td>\n",
       "      <td>-882.142288</td>\n",
       "      <td>-951.993607</td>\n",
       "      <td>-849.592278</td>\n",
       "      <td>-1097.668182</td>\n",
       "      <td>-999.409454</td>\n",
       "      <td>-1042.583557</td>\n",
       "      <td>-706.629913</td>\n",
       "      <td>-775.325386</td>\n",
       "      <td>-717.968155</td>\n",
       "      <td>-1260.845535</td>\n",
       "      <td>-716.319679</td>\n",
       "      <td>-695.200722</td>\n",
       "      <td>-1074.272980</td>\n",
       "      <td>-857.710587</td>\n",
       "      <td>-994.361237</td>\n",
       "      <td>-779.886719</td>\n",
       "      <td>-927.067917</td>\n",
       "      <td>-791.870964</td>\n",
       "      <td>...</td>\n",
       "      <td>-837.701500</td>\n",
       "      <td>-800.745590</td>\n",
       "      <td>-862.521927</td>\n",
       "      <td>-495.854095</td>\n",
       "      <td>-1247.255157</td>\n",
       "      <td>-959.870453</td>\n",
       "      <td>-880.698044</td>\n",
       "      <td>-1165.963379</td>\n",
       "      <td>-988.120651</td>\n",
       "      <td>-690.066666</td>\n",
       "      <td>-832.321800</td>\n",
       "      <td>-565.898743</td>\n",
       "      <td>-862.585396</td>\n",
       "      <td>-842.955948</td>\n",
       "      <td>-884.109314</td>\n",
       "      <td>-971.306091</td>\n",
       "      <td>-1063.918335</td>\n",
       "      <td>-813.979340</td>\n",
       "      <td>-708.132401</td>\n",
       "      <td>-949.787277</td>\n",
       "      <td>-913.287323</td>\n",
       "      <td>110.141963</td>\n",
       "      <td>-966.948944</td>\n",
       "      <td>-648.921509</td>\n",
       "      <td>-1037.581116</td>\n",
       "      <td>-46.994510</td>\n",
       "      <td>-934.003296</td>\n",
       "      <td>-703.262489</td>\n",
       "      <td>-759.569901</td>\n",
       "      <td>-642.749252</td>\n",
       "      <td>-684.248100</td>\n",
       "      <td>-815.427162</td>\n",
       "      <td>-1112.558838</td>\n",
       "      <td>-803.455154</td>\n",
       "      <td>-809.869141</td>\n",
       "      <td>-1009.838821</td>\n",
       "      <td>-703.740692</td>\n",
       "      <td>-926.487900</td>\n",
       "      <td>-969.719452</td>\n",
       "      <td>-844.489319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fname        ...          Zipper_(clothing)\n",
       "0  000ccb97.wav000ccb97.wav000ccb97.wav000ccb97.w...        ...                -183.735697\n",
       "1  0012633b.wav0012633b.wav0012633b.wav0012633b.w...        ...                -662.645607\n",
       "2  001ed5f1.wav001ed5f1.wav001ed5f1.wav001ed5f1.w...        ...                -570.449921\n",
       "3  00294be0.wav00294be0.wav00294be0.wav00294be0.w...        ...                -591.613785\n",
       "4  003fde7a.wav003fde7a.wav003fde7a.wav003fde7a.w...        ...                -844.489319\n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample_subcv = sample_subcv / 5\n",
    "sample_subcv.to_csv('submission.csv', index=False)\n",
    "sample_subcv.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
